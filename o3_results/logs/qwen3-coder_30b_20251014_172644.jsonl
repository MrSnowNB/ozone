{"timestamp": "2025-10-14T17:26:29.325422", "run_id": "qwen3-coder:30b_4096_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36453, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:29.431769", "run_id": "qwen3-coder:30b_4096_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36472, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:29.536339", "run_id": "qwen3-coder:30b_4096_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36479, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:29.648384", "run_id": "qwen3-coder:30b_4096_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36473, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:29.763665", "run_id": "qwen3-coder:30b_4096_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36473, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:29.872565", "run_id": "qwen3-coder:30b_4096_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36439, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:29.973211", "run_id": "qwen3-coder:30b_4096_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36459, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:30.085079", "run_id": "qwen3-coder:30b_4096_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36459, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:30.191557", "run_id": "qwen3-coder:30b_4096_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36475, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:30.299863", "run_id": "qwen3-coder:30b_4096_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36476, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:30.416469", "run_id": "qwen3-coder:30b_4096_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36476, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:30.521735", "run_id": "qwen3-coder:30b_4096_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36497, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:30.628518", "run_id": "qwen3-coder:30b_4096_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36497, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:30.735185", "run_id": "qwen3-coder:30b_4096_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36467, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:30.841156", "run_id": "qwen3-coder:30b_4096_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36483, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:30.951479", "run_id": "qwen3-coder:30b_4096_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36490, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:31.065025", "run_id": "qwen3-coder:30b_4096_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36500, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:31.170794", "run_id": "qwen3-coder:30b_4096_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36513, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:31.276056", "run_id": "qwen3-coder:30b_4096_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36513, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:31.383198", "run_id": "qwen3-coder:30b_4096_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36546, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:31.498539", "run_id": "qwen3-coder:30b_4096_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36505, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:31.603376", "run_id": "qwen3-coder:30b_4096_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36505, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:31.709336", "run_id": "qwen3-coder:30b_4096_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36525, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:31.813222", "run_id": "qwen3-coder:30b_4096_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 4096, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '4096', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36540, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:31.925452", "run_id": "qwen3-coder:30b_8192_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36545, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:32.040750", "run_id": "qwen3-coder:30b_8192_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36554, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:32.147845", "run_id": "qwen3-coder:30b_8192_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36584, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:32.256638", "run_id": "qwen3-coder:30b_8192_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36552, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:32.363537", "run_id": "qwen3-coder:30b_8192_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36545, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:32.477080", "run_id": "qwen3-coder:30b_8192_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36551, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:32.585904", "run_id": "qwen3-coder:30b_8192_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36552, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:32.692221", "run_id": "qwen3-coder:30b_8192_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36579, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:32.797088", "run_id": "qwen3-coder:30b_8192_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36599, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:32.908918", "run_id": "qwen3-coder:30b_8192_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36599, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:33.020923", "run_id": "qwen3-coder:30b_8192_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36584, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:33.125077", "run_id": "qwen3-coder:30b_8192_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36585, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:33.236303", "run_id": "qwen3-coder:30b_8192_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36582, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:33.348482", "run_id": "qwen3-coder:30b_8192_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36475, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:33.456661", "run_id": "qwen3-coder:30b_8192_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36479, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:33.562911", "run_id": "qwen3-coder:30b_8192_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36479, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:33.673946", "run_id": "qwen3-coder:30b_8192_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36505, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:33.779823", "run_id": "qwen3-coder:30b_8192_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36522, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:33.886562", "run_id": "qwen3-coder:30b_8192_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36483, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:33.994744", "run_id": "qwen3-coder:30b_8192_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36505, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:34.097178", "run_id": "qwen3-coder:30b_8192_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36523, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:34.207941", "run_id": "qwen3-coder:30b_8192_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36526, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:34.319480", "run_id": "qwen3-coder:30b_8192_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36360, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:34.431440", "run_id": "qwen3-coder:30b_8192_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 8192, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '8192', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36339, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:34.538066", "run_id": "qwen3-coder:30b_12288_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36341, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:34.647437", "run_id": "qwen3-coder:30b_12288_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 36202, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:34.754271", "run_id": "qwen3-coder:30b_12288_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35914, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:34.861602", "run_id": "qwen3-coder:30b_12288_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35745, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:34.965280", "run_id": "qwen3-coder:30b_12288_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35761, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:35.069312", "run_id": "qwen3-coder:30b_12288_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35783, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:35.175517", "run_id": "qwen3-coder:30b_12288_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35783, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:35.297390", "run_id": "qwen3-coder:30b_12288_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35724, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:35.405657", "run_id": "qwen3-coder:30b_12288_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35692, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:35.513441", "run_id": "qwen3-coder:30b_12288_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35698, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:35.617051", "run_id": "qwen3-coder:30b_12288_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35716, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:35.721864", "run_id": "qwen3-coder:30b_12288_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35738, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:35.828537", "run_id": "qwen3-coder:30b_12288_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35739, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:35.930302", "run_id": "qwen3-coder:30b_12288_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35763, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:36.036531", "run_id": "qwen3-coder:30b_12288_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35742, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:36.145081", "run_id": "qwen3-coder:30b_12288_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35745, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:36.258595", "run_id": "qwen3-coder:30b_12288_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35729, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:36.362096", "run_id": "qwen3-coder:30b_12288_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35748, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:36.471402", "run_id": "qwen3-coder:30b_12288_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35748, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:36.576480", "run_id": "qwen3-coder:30b_12288_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35775, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:36.683524", "run_id": "qwen3-coder:30b_12288_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35663, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:36.788992", "run_id": "qwen3-coder:30b_12288_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35650, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:36.895450", "run_id": "qwen3-coder:30b_12288_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35643, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:37.010249", "run_id": "qwen3-coder:30b_12288_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 12288, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '12288', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35632, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:37.114422", "run_id": "qwen3-coder:30b_16384_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35634, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:37.216115", "run_id": "qwen3-coder:30b_16384_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35655, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:37.318646", "run_id": "qwen3-coder:30b_16384_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35654, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:37.419789", "run_id": "qwen3-coder:30b_16384_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35656, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:37.521844", "run_id": "qwen3-coder:30b_16384_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35678, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:37.625917", "run_id": "qwen3-coder:30b_16384_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35663, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:37.727232", "run_id": "qwen3-coder:30b_16384_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35683, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:37.834617", "run_id": "qwen3-coder:30b_16384_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35686, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:37.944079", "run_id": "qwen3-coder:30b_16384_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35671, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:38.050614", "run_id": "qwen3-coder:30b_16384_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35675, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:38.153418", "run_id": "qwen3-coder:30b_16384_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35693, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:38.257862", "run_id": "qwen3-coder:30b_16384_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35716, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:38.363621", "run_id": "qwen3-coder:30b_16384_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35716, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:38.467701", "run_id": "qwen3-coder:30b_16384_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35697, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:38.570620", "run_id": "qwen3-coder:30b_16384_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35691, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:38.673385", "run_id": "qwen3-coder:30b_16384_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35694, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:38.779153", "run_id": "qwen3-coder:30b_16384_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35716, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:38.895591", "run_id": "qwen3-coder:30b_16384_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35707, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:39.000449", "run_id": "qwen3-coder:30b_16384_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35707, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:39.103545", "run_id": "qwen3-coder:30b_16384_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35731, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:39.205954", "run_id": "qwen3-coder:30b_16384_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35749, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:39.313220", "run_id": "qwen3-coder:30b_16384_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35753, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:39.416618", "run_id": "qwen3-coder:30b_16384_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35728, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:39.521406", "run_id": "qwen3-coder:30b_16384_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 16384, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '16384', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35751, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:39.633421", "run_id": "qwen3-coder:30b_24576_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35753, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:39.747234", "run_id": "qwen3-coder:30b_24576_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35742, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:39.853173", "run_id": "qwen3-coder:30b_24576_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35766, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:39.958048", "run_id": "qwen3-coder:30b_24576_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35768, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:40.060835", "run_id": "qwen3-coder:30b_24576_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35791, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:40.168870", "run_id": "qwen3-coder:30b_24576_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35770, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:40.277029", "run_id": "qwen3-coder:30b_24576_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35768, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:40.391107", "run_id": "qwen3-coder:30b_24576_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35658, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:40.501448", "run_id": "qwen3-coder:30b_24576_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35645, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:40.611210", "run_id": "qwen3-coder:30b_24576_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35653, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:40.715702", "run_id": "qwen3-coder:30b_24576_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35670, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:40.819042", "run_id": "qwen3-coder:30b_24576_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35651, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:40.924025", "run_id": "qwen3-coder:30b_24576_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35652, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:41.028131", "run_id": "qwen3-coder:30b_24576_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35677, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:41.134476", "run_id": "qwen3-coder:30b_24576_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35694, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:41.240844", "run_id": "qwen3-coder:30b_24576_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35698, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:41.358867", "run_id": "qwen3-coder:30b_24576_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35688, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:41.462792", "run_id": "qwen3-coder:30b_24576_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35709, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:41.570896", "run_id": "qwen3-coder:30b_24576_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35708, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:41.674091", "run_id": "qwen3-coder:30b_24576_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35689, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:41.779910", "run_id": "qwen3-coder:30b_24576_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35705, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:41.885579", "run_id": "qwen3-coder:30b_24576_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35710, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:41.988091", "run_id": "qwen3-coder:30b_24576_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35731, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:42.105603", "run_id": "qwen3-coder:30b_24576_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 24576, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '24576', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35712, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:42.212351", "run_id": "qwen3-coder:30b_32768_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35712, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:42.324920", "run_id": "qwen3-coder:30b_32768_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35701, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:42.445920", "run_id": "qwen3-coder:30b_32768_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35721, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:42.558394", "run_id": "qwen3-coder:30b_32768_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35724, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:42.671988", "run_id": "qwen3-coder:30b_32768_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35760, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:42.782333", "run_id": "qwen3-coder:30b_32768_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35786, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:42.892312", "run_id": "qwen3-coder:30b_32768_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35789, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:43.004648", "run_id": "qwen3-coder:30b_32768_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35779, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:43.105974", "run_id": "qwen3-coder:30b_32768_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35755, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:43.215784", "run_id": "qwen3-coder:30b_32768_8_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35758, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:43.318374", "run_id": "qwen3-coder:30b_32768_8_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35782, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:43.422184", "run_id": "qwen3-coder:30b_32768_8_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 8, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '8', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35801, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:43.525056", "run_id": "qwen3-coder:30b_32768_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35803, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:43.628726", "run_id": "qwen3-coder:30b_32768_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35826, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:43.729299", "run_id": "qwen3-coder:30b_32768_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35824, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:43.831654", "run_id": "qwen3-coder:30b_32768_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35829, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:43.952728", "run_id": "qwen3-coder:30b_32768_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35709, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:44.059642", "run_id": "qwen3-coder:30b_32768_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": true, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', '--f16-kv', 'true', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35690, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:44.167289", "run_id": "qwen3-coder:30b_32768_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35686, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:44.273236", "run_id": "qwen3-coder:30b_32768_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35707, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:44.378767", "run_id": "qwen3-coder:30b_32768_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 256, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '256', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35720, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
{"timestamp": "2025-10-14T17:26:44.485953", "run_id": "qwen3-coder:30b_32768_16_1_0", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35724, "ram_after_mb": null, "concurrency_level": 1, "run_index": 0}
{"timestamp": "2025-10-14T17:26:44.591940", "run_id": "qwen3-coder:30b_32768_16_1_1", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35744, "ram_after_mb": null, "concurrency_level": 1, "run_index": 1}
{"timestamp": "2025-10-14T17:26:44.696331", "run_id": "qwen3-coder:30b_32768_16_1_2", "model": "qwen3-coder:30b", "model_digest": "unknown", "config": {"model": "qwen3-coder:30b", "num_ctx": 32768, "batch": 16, "num_predict": 512, "num_thread": 16, "f16_kv": false, "temperature": 0.2, "top_p": 0.95, "seed": 42}, "success": false, "error": "Command '['ollama', 'run', 'qwen3-coder:30b', '--num-ctx', '32768', '--batch', '16', '--num-predict', '512', '--num-thread', '16', '--temperature', '0.2', '--top-p', '0.95', '--seed', '42', 'def fibonacci(n: int) -> int:\\n    # Generate fibonacci sequence up to n terms']' returned non-zero exit status 1.", "ttft_ms": null, "total_ms": null, "output_tokens": null, "tokens_per_sec": null, "vram_before_mb": null, "vram_after_mb": null, "ram_before_mb": 35718, "ram_after_mb": null, "concurrency_level": 1, "run_index": 2}
